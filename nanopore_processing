#!/bin/bash

# nanopore reads come in fast5 format, in a folder called "fast5".  Inside this folder are a bunch of different subdirectories that have the reads binned.  Can either run the basecalling algorith as an array to do each folder separately (takes less time), or run albacore with the recursive flag which looks into each folder.  From what I can tell, it would take almost 2 weeks to run the basecalling with the recursive flag so its probably better to do the array and concatenate later.

# running the basecalling program Albacore
# array program took < 1 day to complete

#!/bin/bash
#SBATCH -J "albacore_array_run"
#SBATCH -n 1               # Use n cores for one job
#SBATCH -a 0-136
#SBATCH -t 2-00:00                # Runtime in D-HH:MM
#SBATCH -p shared                # Partition to submit to
#SBATCH --mem=24G            # Memory pool for all cores
#SBATCH -o outfile.%A.out	# File to which STDOUT will be written
#SBATCH -e outfile.%A.err	# File to which STDERR will be written
#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user=andrew_gehrke@fas.harvard.edu # Email to which notifications will be sent

# Run
/n/home05/agehrke/.conda/envs/albacore/bin/read_fast5_basecaller.py \
-f FLO-MIN106 \
-k SQK-LSK109 \
-i /n/boslfs/INSTRUMENTS/illumina/Nanopore/AGehrke/20180920_2010_AGehrkeH3/fast5/$SLURM_ARRAY_TASK_ID \
-t 40 \
-s /n/regal/srivastava_lab/gehrke/albacore_output/$SLURM_ARRAY_TASK_ID \
-o fastq \
-q 100000

--------------------------------------------------------------------------------------------------------------

# rest of the current workflow is assuming the basecalling was performed as an array (and thus a seperate folder for each bin with pass/fail inside)

# combine all of the "pass" fastq files from all the different output folders of albacore into one fastq file:

#!/bin/bash
#SBATCH -J "combine_fastq"
#SBATCH -n 1               # Use n cores for one job
#SBATCH -t 0-05:00                # Runtime in D-HH:MM
#SBATCH -p shared                # Partition to submit to
#SBATCH --mem=16G            # Memory pool for all cores
#SBATCH -o outfile.%A.out	# File to which STDOUT will be written
#SBATCH -e outfile.%A.err	# File to which STDERR will be written
#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user=andrew_gehrke@fas.harvard.edu # Email to which notifications will be sent

# Run

for i in {0..136}
do
cd /n/regal/srivastava_lab/gehrke/albacore_output/$i/workspace/pass
cat *.fastq >> /n/regal/srivastava_lab/gehrke/final_albacore_combined.fastq
done

--------------------------------------------------------------------------------------------------------------
# program to get stats on nanopore
# slurm script is sbatch_nanoplot.sh

#!/bin/bash
#SBATCH -J "NanoPlot"
#SBATCH -n 4               # Use n cores for one job
#SBATCH -t 0-12:00                # Runtime in D-HH:MM
#SBATCH -p shared                # Partition to submit to
#SBATCH --mem=24G            # Memory pool for all cores
#SBATCH -o outfile.%A.out	# File to which STDOUT will be written
#SBATCH -e outfile.%A.err	# File to which STDERR will be written
#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user=andrew_gehrke@fas.harvard.edu # Email to which notifications will be sent

# to get to the python environment that has NanoFilt, might need to type "source activate albacore" first
module load Anaconda/5.0.1-fasrc02
source activate albacore

# Run
NanoPlot -t 4 --fastq /n/regal/srivastava_lab/gehrke/final_albacore_combined_COMPLETE.fastq -o /n/regal/srivastava_lab/gehrke/nanplot_output  --plots hex dot

--------------------------------------------------------------------------------------------------------------
# potentially optional: filtering the reads with NanoFilt:
# script is sbatch_nanofilt.sh
# to get to the python environment that has NanoFilt, might need to type "source activate albacore" first

#!/bin/bash
#SBATCH -J "Nanofilt"
#SBATCH -n 1               # Use n cores for one job
#SBATCH -t 2-00:00                # Runtime in D-HH:MM
#SBATCH -p shared                # Partition to submit to
#SBATCH --mem=24G            # Memory pool for all cores
#SBATCH -o outfile.%A.out	# File to which STDOUT will be written
#SBATCH -e outfile.%A.err	# File to which STDERR will be written
#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user=andrew_gehrke@fas.harvard.edu # Email to which notifications will be sent

# Run
# to get to the python environment that has NanoFilt, might need to type "source activate albacore" first
# these parameters remove any read that has a quality score < 10 (-q 10), has a length of < 500 bps (-l 500) and removes the first 50 bps of each read (--headcrop 50)

cat /n/regal/srivastava_lab/gehrke/final_albacore_combined_COMPLETE.fastq | NanoFilt -q 10 -l 500 --headcrop 50 > /n/regal/srivastava_lab/gehrke/trimmed-reads.fastq

--------------------------------------------------------------------------------------------------------------

# with this concatenated fastq file, can map to reference genome using minimap2
# pipes the output to samtools to produce a sorted bam file instead of a sam
# script is sbatch_minimap2bam_FINAL.sh

#!/bin/bash
#SBATCH -J "minimap2bam"
#SBATCH -n 1               # Use n cores for one job
#SBATCH -t 0-12:00                # Runtime in D-HH:MM
#SBATCH -p shared                # Partition to submit to
#SBATCH --mem=24G            # Memory pool for all cores
#SBATCH -o outfile.%A.out	# File to which STDOUT will be written
#SBATCH -e outfile.%A.err	# File to which STDERR will be written
#SBATCH --mail-type=ALL           # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user=andrew_gehrke@fas.harvard.edu # Email to which notifications will be sent

#load module
module load minimap2/2.9-fasrc01
module load samtools

# Run
minimap2 -ax map-ont /net/fs2k02/srv/export/srivastava_lab/share_root/ATAC_data/ATAC_DoveTail_Data/raw_reads_cat/hmi_genome.fa /n/regal/srivastava_lab/gehrke/final_albacore_combin$

samtools index /n/regal/srivastava_lab/gehrke/albacore_output.sorted_FINAL.bam

--------------------------------------------------------------------------------------------------------------

# to get a list of regions that contain NO nanopore coverage:

bedtools bamtobed -i albacore_output.sorted_FINAL.bam > albacore_output.sorted_FINAL.bed

bedtools merge -i albacore_output.sorted_FINAL.bed > albacore_output.sorted_FINAL_MERGE.bed

bedtools genomecov -i albacore_output.sorted_FINAL_MERGE.bed -bga -g /net/fs2k02/srv/export/srivastava_lab/share_root/ATAC_data/ATAC_DoveTail_Data/raw_reads_cat/hm.chrom.sizes > albacore_output.sorted_FINAL_MERGE.bedgraph

less albacore_output.sorted_FINAL_MERGE.bedgraph | grep -w 0$ > albacore_output.sorted_FINAL_MERGE.bedgraph_no_coverage.txt

--------------------------------------------------------------------------------------------------------------

# to identify suspicious regions called by REAPR that DO have nanopore coverage (e.g. are "valid")

# had to take the "no_coverage.bed" file from the last step and paste it into coteditor, and save as plain text (test.txt).  There were ghost positions somewhere in the file and needed coteditor to save it correctly.  

bedtools intersect -v -a /Users/agehrke/Google\ Drive/Srivastava_Lab/Manuscripts/ATAC_Manuscript/111318_Science_resubmission/reapr_results/03.score.ALLFCD.bed -b test.txt > good.txt




